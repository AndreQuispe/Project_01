{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0270458e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import utm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f3e307",
   "metadata": {},
   "source": [
    "# 1. Fact Tables "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12be51fc",
   "metadata": {},
   "source": [
    "### 1.1 ) Progreso Diario DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6648ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory path for fact tables using a Windows path\n",
    "path_ft = r'C:\\Users\\ANDRE\\Desktop\\Data Project (Python - Power BI)\\Raw Data\\Fact Tables'\n",
    "\n",
    "# Replace backslashes with forward slashes in the path for compatibility\n",
    "path_ft = path_ft.replace('\\\\', '/')\n",
    "\n",
    "# Retrieves a list of files in the specified directory\n",
    "ft_files = os.listdir(path_ft)\n",
    "\n",
    "# Initializes an empty DataFrame to store concatenated data\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Loop through each file in the 'ft_files' list\n",
    "for file in ft_files:\n",
    "    # Reads each CSV file, specifying delimiter and encoding\n",
    "    current_file = pd.read_csv(path_ft+'/'+file, sep=';', encoding='latin-1')\n",
    "    \n",
    "    # Unpivots the CSV data from wide to long format\n",
    "    unpivot_cf = current_file.melt(\n",
    "        id_vars=current_file.columns[0:4],  # Keeps the first 4 columns as identifier variables\n",
    "        var_name='fecha',  # Renames the pivoted columns to 'fecha'\n",
    "        value_name='valores'  # Renames the values to 'valores'\n",
    "    )\n",
    "    \n",
    "    # Concatenates the unpivoted DataFrame to the main DataFrame 'df'\n",
    "    df = pd.concat([df, unpivot_cf], axis=0)\n",
    "\n",
    "\n",
    "# Data Cleaning and Transformation:\n",
    "\n",
    "# Filtering out rows where 'valores' column is not null\n",
    "df = df[df.valores.notnull()]\n",
    "\n",
    "# Converting the 'fecha' column from object to datetime format\n",
    "df['fecha'] = pd.to_datetime(df['fecha'], format='%d/%m/%Y')\n",
    "\n",
    "# Sorting the table based on specified columns\n",
    "df = df.sort_values(['cod_colegio', 'meac', 'partida', 'fecha'])\n",
    "\n",
    "# Correcting incorrect values in the 'valores' column (cumulative progress)\n",
    "df['valores'].replace('fao', '0.10', inplace=True)  # Replacing 'fao' with a value lower than 0.1683 or zero\n",
    "df['valores'] = df['valores'].astype(float)  # Converting the 'valores' column from object to float\n",
    "\n",
    "# Adjusting cumulative progress values\n",
    "df['valores'] = df.groupby(['cod_colegio','meac','partida']).valores.cummax()\n",
    "\n",
    "# Calculating cumulative progress based on '%_incidencia'\n",
    "df['av_acumulado'] = round(df['%_incidencia'] * df.valores, 4)\n",
    "\n",
    "# Aggregating daily progress for each module\n",
    "df = df.groupby(['cod_colegio','meac','fecha'], as_index=False).av_acumulado.sum()\n",
    "df['av_acumulado'] = round(df.av_acumulado, 4)\n",
    "\n",
    "# Calculating daily progress\n",
    "df['av_diario'] = df.groupby(['cod_colegio','meac']).av_acumulado.diff().fillna(df.av_acumulado)\n",
    "df['av_diario'] = round(df.av_diario, 4)\n",
    "\n",
    "# Creating a new column with the primary keys for this table, it will be the merge of 'colegio' and 'meac'\n",
    "df['colegio_meac'] = df.cod_colegio.astype(str) +'-' + df.meac.astype(str)\n",
    "\n",
    "# Load the final Fact Table\n",
    "progreso_diario = df.copy()\n",
    "\n",
    "# Defining the directory path for cleaned data storage\n",
    "path_cleaned_data = r'C:\\Users\\ANDRE\\Desktop\\Data Project (Python - Power BI)\\Cleaned Data'\n",
    "\n",
    "# Converting backslashes to forward slashes in the path for compatibility\n",
    "path_cleaned_data = path_cleaned_data.replace('\\\\', '/')\n",
    "\n",
    "# Saving the DataFrame 'df' to a CSV file in the cleaned data directory\n",
    "df.to_csv(path_cleaned_data + '/progreso_diario.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd63fa24",
   "metadata": {},
   "source": [
    "### 1.2) Rendimientos DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "429f60ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the previous table of project performance data, we need to extract the performance of each project.\n",
    "\n",
    "def tiempo_ejecucion_efectivo(group):\n",
    "    \"\"\"\n",
    "    This function filters out projects that have not yet been completed.\n",
    "\n",
    "    Args:\n",
    "        group: A pandas DataFrame groupby object.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame with the filtered projects.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a mask to filter out projects that have not yet been completed.\n",
    "    mask = group[group.av_acumulado >= 0.95].iloc[1:].index\n",
    "\n",
    "    # Filter out the projects that have not yet been completed.\n",
    "    filtered_table = group[~group.index.isin(mask)]\n",
    "\n",
    "    # Return the filtered projects.\n",
    "    return filtered_table\n",
    "\n",
    "# Group the data by colegio and meac.\n",
    "rendimientos = df.groupby(['cod_colegio','meac'], as_index=False).apply(tiempo_ejecucion_efectivo).reset_index(drop=True)\n",
    "\n",
    "# Calculate the average daily performance of each project.\n",
    "rendimientos = rendimientos.groupby(['cod_colegio','meac'], as_index=False).agg(\n",
    "    av_acumulado = ('av_diario','sum'),\n",
    "    rendimiento = ('av_diario','mean'),\n",
    "    dias_en_ejecucion = ('av_diario','count'),\n",
    ")\n",
    "\n",
    "# Create a column that classifies each project as either 'Ejecutado' (completed) or 'En Ejecución' (in progress).\n",
    "rendimientos['estado_ejecucion'] = rendimientos.av_acumulado.apply(lambda row: 'Ejecutado' if row >= 0.95 else 'En Ejecución')\n",
    "\n",
    "# Round the rendimiento column to 4 decimal places.\n",
    "rendimientos['rendimiento'] = round(rendimientos.rendimiento, 4)\n",
    "\n",
    "# Create a column that combines the cod_colegio and meac columns into a single column.\n",
    "rendimientos['colegio_meac'] = rendimientos.cod_colegio.astype(str) + '-' + rendimientos.meac.astype(str)\n",
    "\n",
    "# Round the av_acumulado column to 4 decimal places.\n",
    "rendimientos['av_acumulado'] = round(rendimientos.av_acumulado, 4)\n",
    "\n",
    "# Save the resultados DataFrame to a CSV file.\n",
    "rendimientos.to_csv(path_cleaned_data + '/rendimientos.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d19b37",
   "metadata": {},
   "source": [
    "# 2. Dimension Tables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a18fd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Set the path to the directory containing the dimension tables.\n",
    "path_dt = r'C:\\Users\\ANDRE\\Desktop\\Data Project (Python - Power BI)\\Raw Data\\Dimension Tables'\n",
    "path_dt = path_dt.replace('\\\\', '/')  # Ensure path uses forward slashes for consistency\n",
    "\n",
    "# 2. List all files in the directory and filter out only the CSV files.\n",
    "dt_files = os.listdir(path_dt)  # Get a list of all files in the directory\n",
    "csv_dt_files = [file for file in dt_files if file.endswith('.csv')]  # Keep only CSV files\n",
    "\n",
    "# 3. Create an empty dictionary to store the data from the CSV files.\n",
    "dt_data = {}\n",
    "\n",
    "# 4. Loop through each CSV file and read it into a DataFrame.\n",
    "for file in csv_dt_files:\n",
    "    file_name = file[:-4]  # Extract the file name without the .csv extension\n",
    "    dt_data[file_name] = pd.read_csv(path_dt + '/' + file, sep=';', encoding='latin-1')  # Read the CSV file\n",
    "\n",
    "# 5. Create global variables for each DataFrame in the dictionary.\n",
    "for key, value in dt_data.items():\n",
    "    globals()[key] = value  # Make the DataFrames accessible globally\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483f989a",
   "metadata": {},
   "source": [
    "## 2.1) Colegios DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54e57e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the latitud_longitud column into two separate columns, latitud and longitud.\n",
    "colegios[['latitud', 'longitud']] = colegios['latitud_longitud'].str.split(',', expand=True)\n",
    "\n",
    "\n",
    "# Drop the latitud_longitud column as it is no longer needed.\n",
    "colegios.drop(columns=['latitud_longitud'], inplace=True)\n",
    "\n",
    "# Convert the latitud and longitud columns to float data type.\n",
    "colegios['latitud'] = colegios['latitud'].astype(float)\n",
    "colegios['longitud'] = colegios['longitud'].astype(float)\n",
    "\n",
    "# Calculate the Easting and Northing coordinates for each colegio using UTM projection.\n",
    "for index, row in colegios.iterrows():\n",
    "    easting, northing, _, _ = utm.from_latlon(row.latitud, row.longitud, force_zone_number=18)\n",
    "    colegios.loc[index, 'este'] = easting\n",
    "    colegios.loc[index, 'norte'] = northing\n",
    "\n",
    "    \n",
    "    \n",
    "# Defining execution status of each school based on the execution status of each module within it.\n",
    "# If all the modules are executed, the school is executed; If even one module remains to be executed, the school continues in execution.\n",
    " \n",
    "# Calculate the colegio status by grouping by cod_colegio and estado_ejecucion and counting the number of rows for each group.\n",
    "colegio_status = rendimientos.groupby(['cod_colegio','estado_ejecucion'], as_index=False).estado_ejecucion.value_counts()\n",
    "\n",
    "# Add a new column called compare_to which shows the previous estado_ejecucion for each colegio.\n",
    "colegio_status['compare_to']=colegio_status.groupby('cod_colegio')['estado_ejecucion'].shift(-1).fillna(colegio_status.estado_ejecucion)\n",
    "\n",
    "# Get the first row for each colegio, which contains the latest estado_ejecucion and compare_to values.\n",
    "colegio_status = colegio_status.groupby('cod_colegio',as_index=False)[['estado_ejecucion','compare_to']].first()\n",
    "\n",
    "# Create a new column called stado_ejecucion which is 'En Ejecución' if the current or previous estado_ejecucion is 'En Ejecución', otherwise it is 'Ejecutado'.\n",
    "colegio_status['stado_ejecucion'] = colegio_status.apply(lambda row: 'En Ejecución' if (row.estado_ejecucion == 'En Ejecución') or (row.compare_to == 'En Ejecución') else 'Ejecutado', axis=1)\n",
    "\n",
    "# Keep only the cod_colegio and stado_ejecucion columns.\n",
    "colegio_status = colegio_status.loc[:,['cod_colegio','stado_ejecucion']]\n",
    "\n",
    "# Merge the colegio_status DataFrame with the colegios DataFrame on the cod_colegio column.\n",
    "colegios = colegios.merge(colegio_status, on='cod_colegio', how='left')\n",
    "\n",
    "# Fill any missing values in the stado_ejecucion column with 'Pendiente'.\n",
    "colegios['stado_ejecucion'] = colegios['stado_ejecucion'].fillna('Pendiente')\n",
    "\n",
    "# Rename the stado_ejecucion column to estado_ejecución.\n",
    "colegios.rename(columns={'stado_ejecucion':'estado_ejecución'}, inplace=True)\n",
    "\n",
    "# Rename the estado column to estado_pronied.\n",
    "colegios.rename(columns={'estado': 'estado_pronied'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Save the colegios DataFrame to a CSV file.\n",
    "colegios.to_csv(path_cleaned_data + '/colegios.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304305f8",
   "metadata": {},
   "source": [
    "## 2.2) Contratistas DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07fc74c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Export the updated 'contratistas' DataFrame to a CSV file named 'contratistas.csv' \n",
    "contratistas.to_csv(path_cleaned_data + '/contratistas.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eaa85d4",
   "metadata": {},
   "source": [
    "## 2.3) MEACS DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07da059c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a new DataFrame called meac_status containing only the relevant columns from rendimientos.\n",
    "meac_status = rendimientos.loc[:,['cod_colegio','meac','estado_ejecucion']]\n",
    "\n",
    "# 2. Merge the meacs and meac_status DataFrames on the common columns 'cod_colegio' and 'meac'.\n",
    "meacs = meacs.merge(meac_status, on=['cod_colegio', 'meac'], how='left')\n",
    "\n",
    "# 3. Fill any missing values in the 'estado_ejecucion' column with 'Pendiente'.\n",
    "meacs['estado_ejecucion'] = meacs.estado_ejecucion.fillna('Pendiente')\n",
    "\n",
    "# 4. Combine the 'cod_colegio' and 'meac' columns into a single column called 'colegio_meac'.\n",
    "meacs['colegio_meac'] = meacs.cod_colegio.astype(str) + '-' + meacs.meac.astype(str)\n",
    "\n",
    "# 5. Save the meacs DataFrame to a CSV file named 'meacs.csv' without index and using 'latin-1' encoding.\n",
    "meacs.to_csv(path_cleaned_data + '/meacs.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc5de09",
   "metadata": {},
   "source": [
    "## Residentes DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66c328f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Export the updated 'residentes' DataFrame to a CSV file named 'residente.csv' \n",
    "residentes.to_csv(path_cleaned_data +'/residentes.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8ca106",
   "metadata": {},
   "source": [
    "## Residentes Junior DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "387963a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Export the updated 'residentes_junior' DataFrame to a CSV file named 'residentes_junior.csv' \n",
    "residentes_junior.to_csv(path_cleaned_data+'/residentes_junior.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e9397c",
   "metadata": {},
   "source": [
    "## Plazo de Ejecución DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe2fe5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Drop columns starting from the third column onward in the 'plazo_ejecucion' DataFrame.\n",
    "plazo_ejecucion.drop(columns=plazo_ejecucion.columns[3:], inplace=True)\n",
    "\n",
    "# 2. Convert the 'date' column to datetime format using the specified date format ('%d/%m/%Y').\n",
    "plazo_ejecucion['date'] = pd.to_datetime(plazo_ejecucion['date'], format='%d/%m/%Y')\n",
    "\n",
    "# 3. Derive additional date-related columns:\n",
    "plazo_ejecucion['year'] = plazo_ejecucion['date'].dt.year\n",
    "plazo_ejecucion['month_num'] = plazo_ejecucion['date'].dt.month\n",
    "plazo_ejecucion['month'] = plazo_ejecucion['date'].dt.strftime('%b')\n",
    "plazo_ejecucion['week'] = plazo_ejecucion['date'].dt.strftime('%V').astype(int) - 8\n",
    "plazo_ejecucion['day_num'] = plazo_ejecucion['date'].dt.dayofweek + 1\n",
    "plazo_ejecucion['day'] = plazo_ejecucion['date'].dt.strftime('%a')\n",
    "\n",
    "\n",
    "# 4. Create cumulative_sheduled\n",
    "plazo_ejecucion['programado_acumulado'] = plazo_ejecucion.programado_diario.cumsum()\n",
    "\n",
    "\n",
    "\n",
    "# 5. Export the modified 'plazo_ejecucion' DataFrame to a CSV file named 'plazo_ejecucion.csv' in the specified directory.\n",
    "plazo_ejecucion.to_csv(path_cleaned_data + '/plazo_ejecucion.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef948bf",
   "metadata": {},
   "source": [
    "# 3. Factless tables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6df6a411",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Set the path to the directory containing factless tables and replace backslashes with forward slashes for uniformity in the file path representation.\n",
    "path_factless_tables = r'C:\\Users\\ANDRE\\Desktop\\Data Project (Python - Power BI)\\Raw Data\\Factless Fact Tables'\n",
    "path_factless_tables = path_factless_tables.replace('\\\\', '/')\n",
    "\n",
    "# 2. List all files in the directory and filter out only the CSV files.\n",
    "factless_files = os.listdir(path_factless_tables)\n",
    "\n",
    "# 3. Create an empty dictionary 'factless_data' to store DataFrames loaded from CSV files.\n",
    "factless_data = {}\n",
    "\n",
    "# 4. Iterate through each file in the directory:\n",
    "#    - Check if the file ends with '.csv'.\n",
    "#    - If it's a CSV file, read it into a Pandas DataFrame using ';' as the separator and 'latin-1' as the encoding.\n",
    "#    - Store the DataFrame in the 'factless_data' dictionary with the file name (without extension) as the key.\n",
    "for file in factless_files:\n",
    "    if file.endswith('.csv'):\n",
    "        name_file = file[:-4]\n",
    "        factless_data[name_file] = pd.read_csv(path_factless_tables + '/' + file, sep=';', encoding='latin-1')\n",
    "\n",
    "# 5. Create global variables based on the keys of the 'factless_data' dictionary, assigning them the corresponding DataFrames.\n",
    "for key, value in factless_data.items():\n",
    "    globals()[key] = value\n",
    "\n",
    "# 6. Perform modifications on specific DataFrames:\n",
    "#    - Concatenate 'cod_colegio' and 'meac' columns as 'colegio_meac' in 'asignacion_contratistas' DataFrame.\n",
    "asignacion_contratistas['colegio_meac'] = asignacion_contratistas['cod_colegio'].astype(str) + '-' + asignacion_contratistas['meac'].astype(str)\n",
    "\n",
    "#    - Export 'asignacion_contratistas' DataFrame to a CSV file named 'asignacion_contratistas.csv' in the specified directory.\n",
    "asignacion_contratistas.to_csv(path_cleaned_data + '/asignacion_contratistas.csv', index=False)\n",
    "\n",
    "#    - Export 'asignacion_residentes' DataFrame to a CSV file named 'asignacion_residentes.csv' in the specified directory.\n",
    "asignacion_residentes.to_csv(path_cleaned_data + '/asignacion_residentes.csv', index=False)\n",
    "\n",
    "#    - Concatenate 'cod_colegio' and 'meac' columns as 'colegio_meac' in 'asignacion_residentes_junior' DataFrame.\n",
    "asignacion_residentes_junior['colegio_meac'] = asignacion_residentes_junior['cod_colegio'].astype(str) + '-' + asignacion_residentes_junior['meac'].astype(str)\n",
    "\n",
    "#    - Export 'asignacion_residentes_junior' DataFrame to a CSV file named 'asignacion_residentes_junior.csv' in the specified directory.\n",
    "asignacion_residentes_junior.to_csv(path_cleaned_data + '/asignacion_residentes_junior.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
